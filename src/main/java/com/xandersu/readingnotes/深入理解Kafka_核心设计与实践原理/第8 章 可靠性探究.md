Kafka 中采用了多副本的机制，这是大多数分布式系统中惯用的手法，以此来实现水平扩
展、提供容灾能力、提升可用性和可靠性等。我们对此可以引申出一系列的疑问： Kafka 多副
本之间如何进行数据同步，尤其是在发生异常时候的处理机制又是什么？多副本间的数据一致
性如何解决，基于的一致性协议又是什么？如何确保Kafka 的可靠性？ Kafka 中的可靠性和可
用性之间的关系又如何？
本章从副本的角度切入来深挖Kafka 中的数据一致性、数据可靠性等问题，主要包括副本
剖析、日志同步机制和可靠性分析等内容。

8.1 副本剖析

副本（ Replica ）是分布式系统中常见的概念之一，指的是分布式系统对数据和服务提供的
一种冗余方式。在常见的分布式系统中，为了对外提供可用的服务，我们往往会对数据和服务
进行副本处理。数据副本是指在不同的节点上持久化同一份数据，当某一个节点上存储的数据
丢失时，可以从副本上读取该数据，这是解决分布式系统数据丢失问题最有效的手段。另一类
副本是服务副本，指多个节点提供同样的服务，每个节点都有能力接收来自外部的请求并进行
相应的处理。
组成分布式系统的所有计算机都有可能发生任何形式的故障。一个被大量工程实践所检验
过的“黄金定理” ： 任何在设计阶段考虑到的异常情况， 一定会在系统实际运行中发生，并且
在系统实际运行过程中还会遇到很多在设计时未能考虑到的异常故障。所以，除非需求指标允
许，否则在系统设计时不能放过任何异常情况。


第8 章可靠性探究I 2ss
Kafka 从0.8 版本开始为分区引入了多副本机制，通过增加副本数量来提升数据容灾能力。
同时， Kafka 通过多副本机制实现故障自动转移，在Kafka 集群中某个broker 节点失效的情况
下仍然保证服务可用。在l l 节中我们已经简要介绍过副本的概念，并且同时介绍了与副本相
关的AR 、ISR 、HW 和LEO 的概念，这里简要地复习一下相关的概念：
副本是相对于分区而言的，即副本是特定分区的副本。
一个分区中包含一个或多个副本，其中一个为leader 副本，其余为follower 副本，各
个副本位于不同的broker 节点中。只有leader 副本对外提供服务， follower 副本只负
责数据同步。

分区中的所有副本统称为A R ， 而ISR 是指与leader 副本保持同步状态的副本集合，
当然leader 副本本身也是这个集合中的一员。
LEO 标识每个分区中最后一条消息的下一个位置，分区的每个副本都有自己的LEO ,
ISR 中最小的LEO 即为HW ，俗称高水位，消费者只能拉取到HW 之前的消息。
从生产者发出的一条消息首先会被写入分区的leader 副本，不过还需要等待ISR 集合中的
所有follower 副本都同步完之后才能被认为已经提交， 之后才会更新分区的HW ，进而消费者
可以消费到这条消息。
8.1 .1 失效副本
正常情况下，分区的所有副本都处于ISR 集合中，但是难免会有异常情况发生，从而某些
副本被剥离出ISR 集合中。在ISR 集合之外， 也就是处于同步失效或功能失效（ 比如副本处于
非存活状态）的副本统称为失效副本，失效副本对应的分区也就称为同步失效分区，即
under- replicated 分区。
正常情况下，我们通过katka-topics.sh 脚本的under - replicated - partitions 参数来
显示主题中包含失效副本的分区时结果会返回空。比如我们来查看一下主题topic-partitions 的相
关信息（主题topic”partitions 的信息可以参考4 . 3.1 节的相关内容〉：
```
``[root@nodel kafka 2 . 11-2 . 0.0]# bin /kafka topics . sh --zookeeper lo calhost :
2181/kafka -describe --t op 工c topic partitions under-replicated- partitions
```
读者可以自行验证一下，上面的示例中返回为空。紧接着我们将集群中的brokerld 为2 的
节点关闭，再来执行同样的命令， 结果显示如下：
```
[ root@nodel kafka 2 . 11-2 . 0 . 0] # bin/ kafka - topics . sh -- zookeeper localhost : 2181/
kafka describe -- topic topic-partitions --under-repli cated-partitions
```

286 I 深入理解Kafkα ：核l山设计与实践原理
Topic : top 工c -part 工tions
Top 工c : top 工c -partit工ons
Topic: topic -part 工tions
Part工ti on: 0 Leader : 1 Repl 工cas : 1 , 2 , 0 Isr : 1 , 0
Partition : 1 Leader: OReplicas : 2 , 0 , 1 Isr : 0 , 1
Partition : 2 Leader : OReplicas : 0 , 1 , 2 Isr : 0 , 1
可以看到主题topic-partitions 中的三个分区都为under”replicated 分区，因为它们都有副本
处于下线状态，即处于功能失效状态。
前面提及失效副本不仅是指处于功能失效状态的副本，处于同步失效状态的副本也可以看
作失效副本。怎么判定一个分区是否有副本处于同步失效的状态昵？ Kafka 从0.9.x 版本开始就
通过唯一的broker 端参数replica . lag . time . max . ms 来抉择，当ISR 集合中的一个follower
副本滞后leader 副本的时间超过此参数指定的值时则判定为同步失败，需要将此fo llower 副本
剔除出ISR 集合， 具体可以参考图归。rep lica.lag . time.max.ms 参数的默认值为10000 。
｜臼de『follower 1 follower 2
。。「丁
2 2
3 3
4 4
5 5
6 6
- 7
l四Caughtue;即由Ms_ 2
前键－
1. AR= (leader, followerl, follower2)
2. replicaMaxLagTime帖’ 囱rep lica.I哩.time.max.ms 誓数配置， :u 认为10000)
假设z
1 now - lastCaughtUpTimeMs_l 运repl1CaMaxLagTimeMs
2 n 。w -lasteaughtUp百meMs_2 > repltcaMaxlagl丁meMs
结论：
ISR =(leader. f•。llower抖， follower2 失效
图8-1 失效副本的判定
具体的实现原理也很容易理解，当follow 巳r 副本将leader 副本LE O C LogEndOffset ）之前
的日志全部同步时，则认为该fo llo wer 副本己经追赶上lead er 副本，此时更新该副本的
las tCaughtUpTimeMs 标识。Kafka 的副本管理器会启动一个副本过期检测的定时任务，而这个
定时任务会定时检查当前时间与副本的l astCaughtUpTimeMs 差值是否大于参数
replica . lag . time . max . ms 指定的值。千万不要错误地认为fo llower 副本只要拉取leader
副本的数据就会更新la stCa ughtUpTimeMs o 试想一下，当leader 副本中消息的流入速度大于
follower 副本中拉取的速度时，就算follower 副本一直不断地拉取leader 副本的消息也不能与

第8 章可靠性探究I 2s1
leader 副本同步。如果还将此follower 副本置于I SR 集合中，那么当leader 副本下线而选取此
follower 副本为新的leader 副本时就会造成消息的严重丢失。
Kafka 源码注释中说明了一般有两种情况会导致副本失效：
follower 副本进程卡住，在一段时间内根本没有向leader 副本发起同步请求，比如频繁
的Full GC 。
follower 副本进程同步过慢，在一段时间内都无法追赶上leader 副本，比如1/0 开销过
大。
在这里再补充一点，如果通过工具增加了副本因子（参考4.3.4 节），那么新增加的副本在
赶上leader 副本之前也都是处于失效状态的。如果一个follower 副本由于某些原因（ 比如若机）
而下线，之后又上线，在追赶上leader 副本之前也处于失效状态。
在0.9. x 版本之前， Kafka 中还有另一个参数replica.lag . max . messages （默认值为
4000 ） ， 它也是用来判定失效副本的，当一个follower 副本滞后leader 副本的消息数超过
replica . lag .m ax . messages 的大小时，则判定它处于同步失效的状态。它与
replica . lag . time . max .m s 参数判定出的失效副本取并集组成一个失效副本的集合，从而
进一步剥离出分区的ISR 集合。
不过这个replica .l ag . max . messages参数很难给定一个合适的值，若设置得太大，则
这个参数本身就没有太多意义，若设置得太小则会让fo llo wer副本反复处于同步、未同步、同步
的死循环中，进而又造成ISR集合的频繁伸缩。而且这个参数是broker级别的，也就是说， 对broker
中的所有主题都生效。以默认的值4000 为例，对于消息流入速度很低的主题（比如TPS为10) '
这个参数并无用武之地；而对于消息流入速度很高的主题（比女口T因为20000 ），这个参数的取
值又会引入ISR的频繁变动。所以从0 .9 . x版本开始， Kafka ＠［彻底移除了这一参数，相关的资料
还可以参考KIP 161 o
具有失效副本的分区可以从侧面反映出Kafka 集群的很多问题， 毫不夸张地说：如果只用
一个指标来衡量Kafka，那么同步失效分区（具有失效副本的分区）的个数必然是首选。有关
同步失效分区的更多内容可以参考10.3 节。
8.1.2 ISR 的伸缩
Kafka 在启动的时候会开启两个与ISR 相关的定时任务，名称分别为＂ isr-expiration ”和
“ isr-change-propagation ” 。isr-expiration 任务会周期性地检测每个分区是否需要缩减其ISR 集
合。这个周期和repl 工ca . lag . time.max . ms 参数有关，大小是这个参数值的一半， 默认值
1 https://cwiki.apache.org/confluence/display/KAFKA/KIP-1 6+-+Automated+Replica+Lag+ Tun mg
回

288 I 深入理解Kafka ：核l山设计与实践原理
为5000ms 。当检测到ISR 集合中有失效副本时，就会收缩ISR 集合。如果某个分区的ISR 集合
发生变更，则会将变更后的数据记录到ZooKeep巳r 对应的／ brokers/topics/<topic>/
partition/ <parititon>/state 节点中。节点中的数据示例如下：
{ ” controller epoch " :26 , ” leader ”: 0,"versi on" :l,”leader epoch” : 2,”isr": [0 , l]}
其中controller_epoch 表示当前Kafka 控制器的epoch, leader 表示当前分区的leader
副本所在的broker 的id 编号， version 表示版本号（当前版本固定为1 ) , leader epoch
表示当前分区的leader 纪元， isr 表示变更后的ISR 列表。
除此之外，当ISR 集合发生变更时还会将变更后的记录缓存到isrChangeSet 中，
isr-change-propagation 任务会周期性（固定值为2500ms ）地检查isrChangeSet，如果发现
isrChangeSet 中有ISR 集合的变更记录，那么它会在ZooKe叩er 的／ isr change n ot ificat 工on
路径下创建一个以isr_change＿开头的持久顺序节点（比如／isr_change_ notification/isr change_
0000000000 ），并将isrChangeSet 中的信息保存到这个节点中。Kafka 控制器为／ isr change
notification 添加了一个Watcher，当这个节点中有子节点发生变化时会触发Watcher 的动
作，以此通知控制器更新相关元数据信息井向它管理的broker 节点发送更新元数据的请求，最
后删除／ isr change not ifi cation 路径下已经处理过的节点。频繁地触发Watcher 会影响
Kafka 控制器、ZooKeeper 甚至其他broker 节点的性能。为了避免这种情况， Kafka 添加了限定
条件，当检测到分区的ISR 集合发生变化时，还需要检查以下两个条件：
(1 ）上一次ISR 集合发生变化距离现在己经超过缸。
( 2 ）上一次写入ZooKeeper 的时间距离现在已经超过60s 。
满足以上两个条件之一才可以将ISR 集合的变化写入目标节点。
有缩减对应就会有扩充，那么Kafka 又是何时扩充ISR 的呢？
随着follower 副本不断与leader 副本进行消息同步， follower 副本的LEO 也会逐渐后移，
并最终追赶上leader 副本，此时该follower 副本就有资格进入ISR 集合。追赶上leader 副本的
判定准则是此副本的LEO 是否不小于leader 副本的HW ，注意这里并不是和leader 副本的LEO
相比。ISR 扩充之后同样会更新ZooKeeper 中的／ brokers/topics/<topic>/partition/
<p arititon> /state 节点和isrChangeSet，之后的步骤就和ISR 收缩时的相同。
当ISR 集合发生增减时，或者ISR 集合中任一副本的LEO 发生变化时，都可能会影响整个
分区的HW 。
如图8 -2 所示， leader 副本的LEO 为9, followerl 副本的LEO 为7 ，而follower2 副本的
LEO 为6，如果判定这3 个副本都处于ISR 集合中，那么这个分区的HW 为6 ；如果follower3
问工q

第8 章可靠性探究I 289
已经被判定为失效副本被剥离出JSR 集合，那么此时分区的HW 为leader 副本和fo llower! 副本
中LEO 的最小值，即为7 。
leader follower 1 follower 2
。。。
2 2 2
3 3 3
4 4 4
5 5 5
6 1旦，i 6
一LEO • 『,, 、电悄衅’全民《A『··’引1
ι」」啊啊霄~－事肝－呵帽常啡时
8
＿...！－E一 ~」~－－－一J曰晴－’”明一J

图8-2 HW 的变更
冷门知识：很多读者对Kafka 中的HW 的概念并不陌生，但是却并不知道还有一个LW 的
概念。LW 是Low Watermark 的缩写，俗称“低水位”，代表AR 集合中最小的logStartOffset
值。昌1］本的拉取请求（ FetchRequest ，它有可能触发新建日志分段而旧的被清理，进而导致
logStartOffset 的增加）和删除消息请求（ DeleteRecordRequest ）都有可能促使LW 的增长。
8.1.3 LEO 与HW
对于副本而言，还有两个概念：本地副本（ Local Replica ）和远程副本（ Remote Replica) ,
本地副本是指对应的Log 分配在当前的broker 节点上，远程副本是指对应的Log 分配在其他的
broker 节点上。在Kafka 中，同一个分区的信息会存在多个broker 节点上，并被其上的副本管
理器所管理，这样在逻辑层面每个broker 节点上的分区就有了多个副本，但是只有本地副本才
有对应的日志。如图8-3 所示，某个分区有3 个副本分别位于brokerO 、brokerl 和broker2 节点
中，其中带阴影的方框表示本地副本。假设brokerO 上的副本I 为当前分区的leader 副本，那么
副本2 和副本3 就是follower 副本，整个消息追加的过程可以概括如下：
( 1 ）生产者客户端发送消息至leader 副本（副本l) 中。
(2 ）消息被迫加到leader 副本的本地日志，并且会更新日志的偏移量。
( 3) follower 副本（副本2 和副本3 ）向leader 副本请求同步数据。
(4) leader 副本所在的服务器读取本地日志，并更新对应拉取的fo ll ower 副本的信息。


290 I 深入理解Kafka ：核l山设计与实践原理
(5) leader 副本所在的服务器将拉取结果返回给fo llower 副本。
( 6) follower 副本收到l巳ader 副本返回的拉取结果，将消息追加到本地日志中，并更新日
志的偏移量信息。


图8-3 本地副本与远程副本
了解了这些内容后，我们再来分析在这个过程中各个副本LEO 和HW 的变化情况。下面的
示例采用同图8 -3 中相同的环境背景，如图8-4 所示，生产者一直在往leader 副本（带阴影的方
框）中写入消息。某一时刻， leader 副本的LEO 增加至5 ，并且所有副本的HW 还都为0 。
之后fo llower 副本（不带阴影的方框）向leader 副本拉取消息，在拉取的请求中会带有自
身的LEO 信息，这个LEO 信息对应的是FetchRequest 请求中的fetch o ffset o leader 副本
返回给follower 副本相应的消息，并且还带有自身的HW 信息，如图8-5 所示，这个HW 信息
对应的是FetchResponse 中的high_watermark 。
HW=O
图8 -4 情形1 （初始状态） 图8 -5 ’情形2
此时两个follower 副本各自拉取到了消息，并更新各自的LEO 为3 和4 。与此同时， follower
副本还会更新自己的HW ，更新HW 的算法是比较当前LEO 和leader 副本中传送过来的HW 的


第8 章可靠性探究I 291
值， 取较小值作为自己的HW 值。当前两个fo llower 副本的HW 都等于0 ( min(O,O) = 0 ） 。
接下来fo llower 副本再次请求拉取leader 副本中的消息， 如图8 - 6 所示。
此时leader 副本收到来自fol l ower 副本的FetchRequest 请求， 其中带有LEO 的相关信息，
选取其中的最小值作为新的HW, l:IP mi n(l5 , 3,4）斗。然后连同消息和HW 一起返回FetchRe sponse
给fo llower 副本，如图8 -7 所示。注意l eader 副本的HW 是一个很重要的东西，因为它直接影
响了分区数据对消费者的可见性。
LE0 =4
图8-6 情形3 图8 -7 情形4
两个fo ll ower 副本在收到新的消息之后更新LEO 并且更新自己的HW 为3 ( min(LE0 , 3)=3 ） 。
在一个分区中， leader 副本所在的节点会记录所有副本的LEO ， 而fo llower 副本所在的节
点只会记录自身的LEO ，而不会记录其他副本的LEO 。对HW 而言，各个副本所在的节点都只
记录它自身的HW 。变更图8 -3 ，使其带有相应的LEO 和HW 信息，如图8 -8 所示。leader 副
本中带有其他fo llower 副本的LEO ， 那么它们是什么时候更新的呢？ l eader 副本收到fo ll ower
副本的FetchRequest 请求之后， 它首先会从自己的日志文件中读取数据，然后在返回给fo llower
副本数据前先更新follower 副本的LEO 。
, brokerO 副本1 LE创HW: v 问L吨| (
~ ~1J本2 LEO:v Hw:x I
斗－目1~！~~~~~~，~~： 哩＿＿j leader副本j
’ 才副本1 LEO:’ X HW:X j
υ士区转斗画体2 压O v HW: v H Log 川
, 气副本3 :X : I folio＇四r副革I
, broker 2 ………………··- ………- -: lollower/iill 本l 才胁1 旺川川|
i 副本2 LEO: X HW:X i
副本3 L卢：甘即同Log 1J

图8-8 LEO 和HW 在各个副本中的维护情况


2n I 深入理解Kafka ： 核l山设计与实践原理
在图5 -2 中， Kafka 的根目录下有cleaner -offset -checkpoint 、log- start-offset-checkpoint 、
re covery-point-offset-checkpoint 和r eplication ”offs et- checkpo int 四个检查点文件，除了在5.4.2 节
中提及了cleaner-offset-checkpoint ， 其余章节都没有做过多的说明。
recove叩－point - offset-checkpoint 和replication-offset-checkpoint 这两个文件分别对应了LEO
和HW 。Kafka 中会有一个定时任务负责将所有分区的LE O 刷写到恢复点文件recovery-pointoffset-
checkpoint 中， 定时周期由broker 端参数log.flush . offset . checkpoint .
interval . ms 来配置，默认值为60000 。还有一个定时任务负责将所有分区的HW 刷写到复
制点文件replication - offset- checkpoint 中， 定时周期由broker 端参数replica . high . watermark .
checkpoint.interval . ms 来配置，默认值为5000 。
log-start-offset-ch eckpoi nt 文件对应lo gS tartO ffset （注意不能缩写为LSO ，因为在Kafka 中
LSO 是LastStableOffset 的缩写），这个在5.4.1 节中就讲过，在FetchRequest 和Fet chResponse
中也有它的身影，它用来标识日志的起始偏移量。各个副本在变动LEO 和HW 的过程中，
lo g StartOffset 也有可能随之而动。Kafka 也有一个定时任务来负责将所有分区的logStartOffse t
书写到起始点文件lo g-start-offset - checkpoint 中，定时周期由broke r 端参数log.flush . start .
offset . checkpoint . interval . ms 来配置，默认值为6 0000 。
8.1.4 Leader Epoch 的介入
8.1.3 节的内容所陈述的都是在正常情况下的le ad巳r副本与follower副本之间的同步过程，如
果lead er 副本发生切换，那么同步过程又该如何处理呢？在0. 11 . 0.0 版本之前， Kafka使用的是基
于HW 的同步机制，但这样有可能出现数据丢失或leader副本和fo llower副本数据不一致的问题。l
首先我们来看一下数据丢失的问题，如图8 -9 所示， Replica B 是当前的leader 副本（用L
标记） , Replica A 是fo ll ower 副本。参照8 .1.3 节中的图8 -4 至图8 - 7 的过程来进行分析： 在某
一时刻， B 中有2 条消息ml 和m2, A 从B 中同步了这两条消息，此时A 和B 的LE O 都为2,
同时HW 都为l ： 之后A 再向B 中发送请求以拉取消息， F etchReq uest 请求中带上了A 的LE O
信息， B 在收到请求之后更新了自己的HW 为2; B 中虽然没有更多的消息， 但还是在延时一
段时间之后（参考6.3 节中的延时拉取〉返回Fe t chRespon se ，并在其中包含了HW 信息： 最后
A 根据FetchResponse 中的HW 信息更新自己的HW 为2 。
I 参考KIP! 0 I : http s://cwiki.apache.orgiconfluence/d isplay/ KAFKA/K.IP I 0 I +-+A li e什Rep l i cation+Protocol +to+u se+Lea der+ Epoch +
rather+tb an+High+ Watennark+fo r+ Tr un cation 。
民
如
Replica A
。ml
HW
m2
第8 章可靠性探究I 293
Replica B
。ml
m2
HW
图8 -9 数据丢失场景（ part I )
可以看到整个过程中两者之间的HW 同步有一个问隙，在A 写入消息m2 之后C LEO 更新
为2 ）需要再一轮的FetchRequest/ FetchR巳sponse 才能更新自身的HW 为2 。如图8 - 10 所示，如
果在这个时候A 岩机了，那么在A 重启之后会根据之前HW 位置（这个值会存入本地的复制
点文件replication - offs巳t- checkpoint ）进行日志截断， 这样便会将m2 这条消息删除，此时A 只
剩下ml 这一条消息，之后A 再向B 发送FetchRe quest 请求拉取消息。
Rep li ca A
”叭，
ml
Replica B
。ml
m2
HW
图8 -10 数据丢失场景（ part 2 )
此时若B 再右机，那么A 就会被选举为新的leader ， 如图8 -1 l 所示。B 恢复之后会成为
follower ，由于follower 副本HW 不能比leader 副本的HW 高，所以还会做一次日志截断，以此
将HW 调整为l 。这样一来m2 这条消息就丢失了（就算B 不能恢复， 这条消息也同样丢失）。
Replica A
HW
O ml
Truncated !
Replica B
图8- 11 数据丢失场景（ part 3 )
对于这种情况，也有一些解决方法，比如等待所有fo llower 副本都更新完自身的HW 之后
再更新le ader 副本的HW ，这样会增加多一轮的F etchRequest/ F etchRespo nse 延迟， 自然不够妥
当。还有一种方法就是follower 副本恢复之后，在收到leader 副本的F etchResponse 前不要截断
follow巳r 副本（ follower 副本恢复之后会做两件事情：截断自身和向l eader 发送FetchReques t 请
求），不过这样也避免不了数据不一致的问题。
如图8-12 所示，当前leader 副本为A, follower 副本为B , A 中有2 条消息m l 和m2 ，并
且HW 和LEO 都为2, B 中有1 条消息ml ， 井且HW 和LEO 都为l 。假设A 和B 同时“挂掉”，
民
如
294 I 深入理解Kafka ：核l山设计与实践原理
HW
Repliεa A
然后B 第一个恢复过来并成为leader ，如图8-13 所示。
Replica B
ml
m2
HW
。ml
Replica A
图8-12 数据不一致场景（ p art 1 )
Replica B
町、1
mz
HW
”、1 HW
图8- 13 数据不一致场景（ part 2)
之后B 写入消息m3 ， 并将LEO 和HW 更新至2 （假设所有场景中的rnin . insync.replicas
参数配置为1 ） 。此时A 也恢复过来了，根据前面数据丢失场景中的介绍可知它会被赋予follower
的角色，井且需要根据HW 截断日志及发送FetchRequest 至B ，不过此时A 的HW 正好也为2,
那么就可以不做任何调整了，如图8-14 所示。
Replica A
HW
。ml
mZ
Replica B
HW
。ml
m3
图8-14 数据不一致场景（ part 3)
如此一来A 中保留了m2 而B 中没有， B 中新增了m3 而A 也同步不到，这样A 和B 就出
现了数据不一致的情形。
为了解决上述两种问题， Kafka 从0.11.0.0 开始引入了leader epoch 的概念，在需要截断数
据的时候使用leader epoch 作为参考依据而不是原本的HW 。leader epoch 代表leader 的纪元信
息（ epoch ），初始值为0 。每当leader 变更一次， leader epoch 的值就会加l ，相当于为leader
增设了一个版本号。与此同时，每个副本中还会增设一个矢量＜LeaderEpoch => StartOffset＞，其
中StartOffset 表示当前LeaderEpoch 下写入的第一条消息的偏移量。每个副本的Log 下都有一
个leader-epoch-checkpoint 文件，在发生leader epoch 变更时，会将对应的矢量对追加到这个文
件中，其实这个文件在图5 -2 中己有所呈现。5 .2.5 节中讲述v2 版本的消息格式时就提到了消息
集中的partition leader epoch 宇段，而这个字段正对应这里讲述的leader epoch 。
民如
第8 章可靠性探究I 29s
下面我们再来看一下引入leader epoch 之后如何应付前面所说的数据丢失和数据不一致的
场景。首先讲述应对数据丢失的问题，如图8 - 15 所示，这里只比图8 -9 中多了LE (Leader Epoch
的缩写，当前A 和B 中的LE 都为0 ） 。
Re 口lica. A Replica B
HW
m2
。ml LEO
HW
1 m2 LEO
O ml
LE O忏set
0 0
LE O忏set
0 0
图8-15 应对数据丢失（ part I )
同样A 发生重启，之后A 不是先忙着截断日志而是先发送OffsetsFor Leader EpochRequest
请求给B ( OffsetsForLeaderEpochRequest 请求体结构如图也16 所示，其中包含A 当前的
Leader Epoch 值） ' B 作为目前的leader 在收到请求之后会返回当前的LEO ( LogEndOffset ，注
意图中LEO 和LEO 的不同），与请求对应的响应为OffsetsForLeaderEpochResponse ，对应的响
应体结构可以参考图8 - 17 ， 整个过程可以参考图8 -1 8 。
.
’E
EE
EEEE
E‘.
Requests。dy
飞EtEE 『EE -
，
,
a
-….‘··
~j to阳「
仁~丁partitions1
int32 int32 溢
I part阳
图8 - 16 OffsetsF orLeaderEpochRequest 请求体结构
如果A 中的LeaderEpoch （假设为LE_A ）和B 中的不相同，那么B 此时会查找LeaderEpoch
为LE A+l 对应的StartOffset 并返回给A ，也就是LE A 对应的LEO ，所以我们可以将
OffsetsF or LeaderEpochRequest 的请求看作用来查找fo llower 副本当前LeaderEpoch 的LEO 。
回
296 I 深入理解Kafka ：如山设计与实践原理
」Respon世Body
L主J
E二~.. -
, , , F’h ’n’ uc u ’ int32 int32 int64' ‘句
、
error code partition I leader ep。ch I end＿。何set
图8 -17 OffsetsF orLeaderEpochResponse 响应体结构
Replica A
HW
LE O忏set
。。
。何se』Forleader
Epoch Request
Replica B
LEO
-+ I O ml
+ --
2
m2
HW
LE O怦set
0 0
图8 -1 8 反对数据丢失（ part 2 )
如图8-18 所示， A 在收到2 之后发现和目前的LEO 相同，也就不需要截断日志了。之后
同图8-11 所示的一样， B 发生了右机， A 成为新的leader，那么对应的LE=O 也变成了LE= l,
对应的消息m2 此时就得到了保留，这是原本图8 -11 中所不能的，如图8 -19 所示。之后不管B
有没有恢复，后续的消息都可以以LE l 为LeaderEpoch 陆续追加到A 中。
II Rep Iiεa B
。ml LEO
m2 LEO
2 m3 LEl
LE Offset LE Offset
。。。。
2
图8- 1 9 应对数据丢失（ part 3 )
下面我们再来看一下leader epoch 如何应对数据不一致的场景。如图8 -20 所示，当前A 为
leader, B 为fo llower, A 中有2 条消息ml 和m2 ，而B 中有1 条消息ml 。假设A 和B 同时“挂
掉”，然后B 第一个恢复过来并成为新的leader 。
回
版权所有，严禁传播，违者白负法律责任！
第8 章可靠性探究I 291
Replica A Replica B
"Q ml 町、1 LEO
HW
m2 L臼
HW
LE Offset LE Offset
口。0 0
图8-20 应对数据不一致（ part 1 )
之后B 写入消息m3 ， 并将LEO 和HW 更新至2 ，如图8-21 所示。注意此时的LeaderEpoch
己经从LEO 增至LEI 了。
Replica A Replica B
ml 。内、1
H飞N
m2 m3 LEl
甲
LE Off’ 'set LE Offset
。。。。
图8 -21 应对数据不一致（ part 2 )
紧接着A 也恢复过来成为follower 并向B 发送OffsetsForLeaderEpochRequest 请求，此时A
的LeaderEpoch 为LEO 。B 根据LEO 查询到对应的offset 为1 井返回给A, A 就截断日志并删
除了消息m2 ，如图8 -22 所示。之后A 发送FetchRequest 至B 请求来同步数据， 最终A 和B
中都有两条消息ml 和m3, HW 和LEO 都为2 ，井且LeaderEpoch 都为LEI ，如此便解决了数
据不一致的问题。
Offsetsforleader
Ep。chReq uest
Replica A Replica B
LEO
。ml
____.
ml
+i-
’·－吨’ m m3
HW HW
m3 LEl
LE Offset LE Offset
0 0 0
图8 -22 应对数据不一致（ part 3)
8.1.5 为什么不支持读写分离
在Kafka 中，生产者写入消息、消费者读取消息的操作都是与leader 副本进行交互的，从
而实现的是一种主写主读的生产消费模型。数据库、Redis 等都具备主写主读的功能， 与此同时
卧
版权所有，严禁传播，违者自负法律责任！
29s I 深入理解Kafka ：核l山设计与实践原理
还支持主写从读的功能，主写从读也就是读写分离，为了与主写主读对应，这里就以主写从读
来称呼。Kafka 并不支持主写从读，这是为什么呢？
从代码层面上来说，虽然增加了代码复杂度，但在Kafka 中这种功能完全可以支持。对于
这个问题，我们可以从“收益点”这个角度来做具体分析。主写从读可以让从节点去分担主节
点的负载压力，预防主节点负载过重而从节点却空闲的情况发生。但是主写从读也有2 个很明
显的缺点：
(1 ）数据一致性问题。数据从主节点转到从节点必然会有一个延时的时间窗口，这个时间
窗口会导致主从节点之间的数据不一致。某一时刻，在主节点和从节点中A 数据的值都为x,
之后将主节点中A 的值修改为Y，那么在这个变更通知到从节点之前， 应用读取从节点中的A
数据的值并不为最新的Y，由此便产生了数据不一致的问题。
( 2 ）延时问题。类似Redis 这种组件，数据从写入主节点到同步至从节点中的过程需要经
历网络→主节点内存→网络→从节点内存这几个阶段，整个过程会耗费一定的时间。而在Kafka
中，主从同步会比Redi s 更加耗时，它需要经历网络→主节点内存→主节点磁盘→网络→从节
点内存→从节点磁盘这几个阶段。对延时敏感的应用而言， 主写从读的功能并不太适用。
现实情况下，很多应用既可以忍受一定程度上的延时， 也可以忍受一段时间内的数据不一
致的情况，那么对于这种情况， Kafka 是否有必要支持主写从读的功能呢？
主读从写可以均摊一定的负载却不能做到完全的负载均衡，比如对于数据写压力很大而读
压力很小的情况，从节点只能分摊很少的负载压力，而绝大多数压力还是在主节点上。而在Kafka
中却可以达到很大程度上的负载均衡，而且这种均衡是在主写主读的架构上实现的。我们来看
一下Kafka 的生产消费模型，如图8 -2 3 所示。
消费者
图8 -23 Kafka 的生产消费模型
如图8 -23 所示，在Kafka 集群中有3 个分区，每个分区有3 个副本，正好均匀地分布在3
问~9
＼干乃／γ
版权所有，严禁传播，违者臼负法律责任！
第8 章可靠性探究I 299
个broker 上，灰色阴影的代表leader 副本，非灰色阴影的代表follower 副本，虚线表示follower
副本从leader 副本上拉取消息。当生产者写入消息的时候都写入leader 副本，对于图8 -23 中的
情形，每个broker 都有消息从生产者流入；当消费者读取消息的时候也是从leader 副本中读取
的，对于图8 -23 中的情形，每个broker 都有消息流出到消费者。
我们很明显地可以看出， 每个broker 上的读写负载都是一样的，这就说明Kafka 可以通过
主写主读实现主写从读实现不了的负载均衡。图8 -23 展示是一种理想的部署情况，有以下几种
情况（包含但不仅限于〉会造成一定程度上的负载不均衡：
( I ) broker 端的分区分配不均。当创建主题的时候可能会出现某些broker 分配到的分区数
多而其他broker 分配到的分区数少，那么自然而然地分配到的leader 副本也就不均。
(2 ）生产者写入消息不均。生产者可能只对某些broker 中的leader 副本进行大量的写入操
作，而对其他broker 中的leader 副本不闻不问。
(3 ）消费者消费消息不均。消费者可能只对某些broker 中的leader 副本进行大量的拉取操
作，而对其他broker 中的leader 副本不闻不问。
(4) leader 副本的切换不均。在实际应用中可能会由于broker 岩机而造成主从副本的切换，
或者分区副本的重分配等，这些动作都有可能造成各个broker 中leader 副本的分配不均。
对此，我们可以做一些防范措施。针对第一种情况，在主题创建的时候尽可能使分区分配
得均衡，好在Kafka 中相应的分配算法也是在极力地追求这一目标，如果是开发人员自定义的
分配，则需要注意这方面的内容。对于第二和第三种情况，主写从读也无法解决。对于第四种
情况， Kafka 提供了优先副本的选举来达到leader 副本的均衡， 与此同时，也可以配合相应的
监控、告警和运维平台来实现均衡的优化。
在实际应用中，配合监控、告警、运维相结合的生态平台，在绝大多数情况下Kafka 都能
做到很大程度上的负载均衡。总的来说， Kafka 只支持主写主读有几个优点：可以简化代码的
实现逻辑，减少出错的可能；将负载粒度细化均摊，与主写从读相比，不仅负载效能更好，而
且对用户可控；没有延时的影响；在副本稳定的情况下，不会出现数据不一致的情况。为此，
Kafka 又何必再去实现对它而言毫无收益的主写从读的功能呢？这一切都得益于Kafka 优秀的
架构设计，从某种意义上来说，主写从读是由于设计上的缺陷而形成的权宣之计。
8.2 日志同步机制
在分布式系统中，日志同步机制既要保证数据的一致性，也要保证数据的顺序性。虽然有
许多方式可以实现这些功能，但最简单高效的方式还是从集群中选出一个leader 来负责处理数
据写入的！｜说序性。只要leader 还处于存活状态，那么follower 只需按照leader 中的写入顺序来
进行同步即可。
回
版权所有，严禁传播，违者白负法律责任！
300 I 深入理解Kafka ：核J山设计与实践原理
通常情况下，只要leader 不着机我们就不需要关心fo llower 的同步问题。不过当l eader 岩
机时，我们就要从fo llower 中选举出一个新的leader 。follower 的同步状态可能落后leader 很多，
甚至还可能处于窑机状态，所以必须确保选择具有最新日志消息的fo llower 作为新的leader 。日
志同步机制的一个基本原则就是： 如果告知客户端已经成功提交了某条消息，那么即使leader
岩机，也要保证新选举出来的leader 中能够包含这条消息。这里就有一个需要权衡（ tradeoff)
的地方，如果leader 在消息被提交前需要等待更多的follower 确认，那么在它岩机之后就可以
有更多的fo llower 替代它，不过这也会造成性能的下降。
对于这种tradeo筐， 一种常见的做法是“少数服从多数”，它可以用来负责提交决策和选举
决策。虽然Kafka 不采用这种方式，但可以拿来探讨和理解tradeoff 的艺术。在这种方式下，如
果我们有2f+ l 个副本，那么在提交之前必须保证有轩1 个副本同步完消息。同时为了保证能正
确选举出新的leader，至少要保证有f+ l 个副本节点完成日志同步井从同步完成的副本中选举出
新的leader 节点。并且在不超过f 个副本节点失败的情况下，新的leader 需要保证不会丢失己
经提交过的全部消息。这样在任意组合的f+ l 个副本中，理论上可以确保至少有一个副本能够
包含己提交的全部消息，这个副本的日志拥有最全的消息，因此会有资格被选举为新的leader
来对外提供服务。
“少数服从多数”的方式有一个很大的优势，系统的延迟取决于最快的几个节点，比如副
本数为3 ， 那么延迟就取决于最快的那个follower 而不是最慢的那个（除了l eader，只需要另一
个fo llower 确认即可〉。不过它也有一些劣势，为了保证leader 选举的正常进行，它所能容忍
的失败follower 数比较少，如果要容忍l 个follower 失败，那么至少要有3 个副本，如果要容
忍2 个follower 失败，必须要有5 个副本。也就是说，在生产环境下为了保证较高的容错率，
必须要有大量的副本，而大量的副本又会在大数据量下导致性能的急剧下降。这也就是“少数
服从多数”的这种Quorum 模型常被用作共享集群配置（ 比如ZooKeeper ），而很少用于主流的
数据存储中的原因。
与“少数服从多数”相关的一致性协议有很多， 比如Zab 、Raft 和Viewstamped Replication
等。而Kafka 使用的更像是微软的PacificA 算法。
在Kafka 中动态维护着一个ISR 集合，处于ISR 集合内的节点保持与leader 相同的高水位
CHW ），只有位列其中的副本（ unclean . leader . elect 工on.enable 配置为fal se ）才有
资格被选为新的leader a 写入消息时只有等到所有ISR 集合中的副本都确认收到之后才能被认
为已经提交。位于ISR 中的任何副本节点都有资格成为leader，选举过程简单（详细内容可以
参考6 .4.3 节〉、开销低，这也是Kafka 选用此模型的重要因素。Kafka 中包含大量的分区， leader
副本的均衡保障了整体负载的均衡，所以这一因素也极大地影响Kafka 的性能指标。
在采用ISR 模型和（ f+ l ）个副本数的配置下，一个Kafka 分区能够容忍最大f 个节点失败，
相比于“少数服从多数”的方式所需的节点数大幅减少。实际上，为了能够容忍f 个节点失败，
回
版权所有，严禁传播，违者自负法律责任！
第8 章可靠性探究I 301
“少数服从多数”的方式和ISR 的方式都需要相同数量副本的确认信息才能提交消息。比如，
为了容忍l 个节点失败，“少数服从多数” 需要3 个副本和l 个follower 的确认信息， 采用ISR
的方式需要2 个副本和l 个follower 的确认信息。在需要相同确认信息数的情况下， 采用ISR
的方式所需要的副本总数变少，复制带来的集群开销也就更低， “少数服从多数”的优势在于
它可以绕开最慢副本的确认信息，降低提交的延迟，而对Kafka 而言，这种能力可以交由客户
端自己去选择。
另外，一般的同步策略依赖于稳定的存储系统来做数据恢复， 也就是说， 在数据恢复时日
志文件不可丢失且不能有数据上的冲突。不过它们忽视了两个问题： 首先， 磁盘故障是会经常
发生的，在持久化数据的过程中并不能完全保证数据的完整性；其次，即使不存在硬件级别的
故障，我们也不希望在每次写入数据时执行同步刷盘（ fsync ）的动作来保证数据的完整性，这
样会极大地影响性能。而Kafka 不需要岩机节点必须从本地数据日志、中进行恢复， Kafka 的同
步方式允许宿机副本重新加入IS R 集合，但在进入ISR 之前必须保证自己能够重新同步完leader
中的所有数据。
8.3 可靠性分析
很多人问过笔者类似这样的一些问题：怎样可以确保Kafka 完全可靠？如果这样做就可以
确保消息不丢失了吗？笔者认为： 就可靠性本身而言，它并不是一个可以用简单的“ 是”或“ 否”
来衡量的一个指标，而一般是采用几个9 来衡量的。任何东西不可能做到完全的可靠，即使能
应付单机故障，也难以应付集群、数据中心等集体故障，即使躲得过天灾也未必躲得过人祸。
就可靠性而言，我们可以基于一定的假设前提来做分析。本节要讲述的是：在只考虑Kafka 本
身使用方式的前提下如何最大程度地提高可靠性。
就Kafka 而言，越多的副本数越能够保证数据的可靠性，副本数可以在创建主题时配置，
也可以在后期修改，不过副本数越多也会引起磁盘、网络带宽的浪费，同时会引起性能的下降。
一般而言，设置副本数为3 即可满足绝大多数场景对可靠性的要求，而对可靠性要求更高的场
景下，可以适当增大这个数值，比如国内部分银行在使用Kafka 时就会设置副本数为5 。与此
同时，如果能够在分配分区副本的时候引入基架信息（ broker.rack 参数） ，那么还要应对
机架整体岩机的风险。
仅依靠副本数来支撑可靠性是远远不够的，大多数人还会想到生产者客户端参数acks 。
在2.3 节中我们就介绍过这个参数：相比于0 和L acks = -1 （客户端还可以配置为a ll ，它的含
义与一l 一样，以下只以1 来进行陈述〉可以最大程度地提高消息的可靠性。
对于acks = 1 的配置，生产者将消息发送到lead er 副本， l eader 副本在成功写入本地日志之
后会告知生产者己经成功提交，如图8 -24 所示。如果此时ISR 集合的fo llo wer 副本还没来得及
拉取到leader 中新写入的消息， leader 就看机了，那么此次发迭的消息就会丢失。
回
版权所有，严禁传播，违者自负法w 或任！
302 I 深入理解Kafka ：核l山设计与实践原理
怆剖er folio明门f。II 。响应leader foll。叭居门folio响晤r2
唰「.，§~~ 。§§ ,, , , , ,
’,’ ,’,,,
- …·
＠消息写入陆“er
副本之后， folio明r
jijj本来往取消息进
行罔步
〈二F (!) Producer写λ消息3和4
3
国LEOI 4
陆ader folio驹en follov.阳2
新恒ader
｛原folio明门） folio明『2
。RE FE
罔甲：剧目国和气@folio明门当盗为新的
悔剧目，但是此时LEO
仍为3 , Producer.发送
的消息4,5丢失
4
③ folio明I还
没有来得及
fetch到恒划凹
的最新消息
陆“er就军Z 视
了
图8 -24 acks=l 的配直情形
对于ack ＝ 一l 的配置，生产者将消息发送到leader 副本， leader 副本在成功写入本地日志之
后还要等待IS R 中的follower 副本全部同步完成才能够告知生产者已经成功提交，即使此时
leader 副本君机，消息也不会丢失，如图8 -25 所示。
leader foll。呐串门fol阳W昭2 隐ader follov.凹T follower2
o 11 o 川0 。门。门。
2
-3·4
2-3·4
2-3·4
③ follov.er全部闰步完成， 更新HW,
返回害户结成功标志
©leader发生窑机， 需要从ISR申盗举
出－个foll。明r成为新的l臼der 不管
是谁当选．消息都不会丢失
图8-25 acks=-1 的配置情形（成功）
同样对于acks = - 1 的配置，如果在消息成功写入leader 副本之后，并且在被ISR 中的所有副
卧
版权所有，严禁传播，违者自负法律责任！
第8 章可靠性探究I 303
本同步之前leader 副本着机了，那么生产者会收到异常以此告知此次发送失败，如图8-26 所示。
leader folio执串门foli o确ier2
。。E
HWILEOI 4
·-- ..
③消息写入l臼der；~本之后， 但在彼folio晒『
副本完全罔步之前就写机了，那么生产者会
收到异常， 告知此时发送失政
图8 -26 acks= 1 的目己直情形（失败）
在2. 1.2 节中，我们讨论了消息发送的3 种模式，即发后即忘、同步和异步。对于发后即忘
的模式，不管消息有没有被成功写入，生产者都不会收到通知，那么即使消息写入失败也无从
得知，因此发后即忘的模式不适合高可靠性要求的场景。如果要提升可靠性，那么生产者可以
采用同步或异步的模式，在出现异常情况时可以及时获得通知，以便可以做相应的补救措施，
比如选择重试发送（可能会引起消息重复）。
有些发送异常属于可重试异常，比如NetworkException，这个可能是由瞬时的网络故障而
导致的， 一般通过重试就可以解决。对于这类异常，如果直接抛给客户端的使用方也未免过于
兴师动众， 客户端内部本身提供了重试机制来应对这种类型的异常，通过ret ri es 参数即可
配置。默认情况下， retries 参数设置为0 ，即不进行重试，对于高可靠性要求的场景， 需要
将这个值设置为大于0 的值，在2.3 节中也谈到了与retries 参数相关的还有一个
retry.back off . ms 参数，它用来设定两次重试之间的时间间隔，以此避免无效的频繁重试。
在配置retries 和retr y . backoff . ms 之前，最好先估算一下可能的异常恢复时间，这样
可以设定总的重试时间大于这个异常恢复时间，以此来避免生产者过早地放弃重试。如果不知
道r e tries 参数应该配置为多少， 则可以参考KafkaAdminClient ，在KafkaAdminClient 中
retries 参数的默认值为5 。
注意如果配置的r etries 参数值大于0 ， 则可能引起一些负面的影响。首先同2.3 节中谈
及的一样，由于默认的max . 川.fl 工ght . requests . per . connection 参数值为5 ，这样可
能会影响消息的顺序性，对此要么放弃客户端内部的重试功能，要么将
ma x .i n . flight . requests . per . connection 参数设置为l ，这样也就放弃了吞吐。其次，
有些应用对于时延的要求很高，很多时候都是需要快速失败的，设置retries> 0 会增加客户
端对于异常的反馈时延，如此可能会对应用造成不良的影响。
我们回头再来看一下acks ＝寸的情形，它要求ISR 中所有的副本都收到相关的消息之后才
卧
版权所有，严禁传播，违者白负法律责任！
304 I 深入理解Kafka ：棋l山设计与实践原理
能够告知生产者己经成功提交。试想一下这样的情形， leader 副本的消息流入速度很快，而
follower 副本的同步速度很慢，在某个临界点时所有的follower 副本都被剔除出了ISR 集合，那
么ISR 中只有一个leader 副本，最终acks ＝一l 演变为acks = 1 的情形，如此也就加大了消息丢
失的风险。Kafka 也考虑到了这种情况，并为此提供了min .i nsync . replicas 参数（默认值
为1 ）来作为辅助（配合acks = - 1 来使用〉，这个参数指定了ISR 集合中最小的副本数，如果
不满足条件就会抛出No tEnoughReplicasException 或NotEnoughReplicasAfter AppendException 。
在正常的配置下，需要满足副本数＞ min. i 口sync.replicas 参数的值。一个典型的配置方
案为：副本数配置为3, min . 工nsync.replicas 参数值配置为2 o 注意min .i nsync .
replicas 参数在提升可靠性的时候会从侧面影响可用性。试想如果ISR 中只有一个leader 副
本，那么最起码还可以使用，而此时如果配置m工n.insync.replicas> l ，则会使消息无法
写入。
与可靠性和ISR 集合有关的还有一个参数一－unclean . leader.election . enable 。
这个参数的默认值为false ，如果设置为true 就意味着当leader 下线时候可以从非ISR 集合中选
举出新的leader ， 这样有可能造成数据的丢失。如果这个参数设置为false ， 那么也会影响可用
性，非ISR 集合中的副本虽然没能及时同步所有的消息，但最起码还是存活的可用副本。随着
Kafka 版本的变更，有的参数被淘汰，也有新的参数加入进来，而传承下来的参数一般都很少
会修改既定的默认值，而unclean.leader.election . enable 就是这样一个反例，从
0.11.0.0 版本开始， unclean.leader . election.enable 的默认值由原来的true 改为了
false ，可以看出Kafka 的设计者愈发地偏向于可靠性的提升。
在broker 端还有两个参数log . flush . interval . messages 和log. flush. interval .ms,
用来调整同步刷盘的策略，默认是不做控制而交由操作系统本身来进行处理。同步刷盘是增强
一个组件可靠性的有效方式， Kafka 也不例外，但笔者对同步刷盘有一定的疑问一一绝大多数
情景下，一个组件（尤其是大数据量的组件）的可靠性不应该由同步刷盘这种极其损耗性能的
操作来保障，而应该采用多副本的机制来保障。
对于消息的可靠性，很多人都会忽视消费端的重要性，如果一条消息成功地写入Kafka,
并且也被Kafka 完好地保存，而在消费时由于某些疏忽造成没有消费到这条消息，那么对于应
用来说，这条消息也是丢失的。
enable.auto.commit 参数的默认值为true，即开启自动位移提交的功能， 虽然这种方
式非常简便，但它会带来重复消费和消息丢失的问题，对于高可靠性要求的应用来说显然不可
取，所以需要将enable . auto . commit 参数设置为false 来执行手动位移提交。在执行手动
位移提交的时候也要遵循一个原则：如果消息没有被成功消费，那么就不能提交所对应的消费
位移。对于高可靠要求的应用来说，宁愿重复消费也不应该因为消费异常而导致消息丢失。有
时候，由于应用解析消息的异常，可能导致部分消息一直不能够成功被消费，那么这个时候为
回
版权所有，严禁传播，违者自负法律责任！
第8 章可靠性探究I 30s
了不影响整体消费的进度，可以将这类消息暂存到死信队列（查看11.3 节）中，以便后续的故
障排除。
对于消费端， Kafka 还提供了一个可以兜底的功能，即回溯消费，通过这个功能可以让我
们能够有机会对漏掉的消息相应地进行回补，进而可以进一步提高可靠性。
8.4 总结
笔者接触Kafka 以来被问得最多的就是Kafka 的可靠性问题，本章以此为引来对Kafka 相
关的知识点进行讲解，最后通过可靠性分析来做一个总结，希望能够为读者在遇到此类问题时
提供参考。